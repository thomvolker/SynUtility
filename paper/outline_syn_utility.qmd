---
title: "OUTLINE: Density ratio estimation as a technique for assessing the utility of synthetic data"
output: html
bibliography: densratio_for_utility.bib
---

# Target outlet

- [JPC](https://journalprivacyconfidentiality.org/index.php/jpc)
- [JOS](https://sciendo.com/journal/JOS)
- [JRSSA](https://rss.onlinelibrary.wiley.com/journal/1467985x)


# Introduction

__Section 1: relevance__

Openly accessible research data can accelerate scientific progress tremendously, by allowing a wide audience of researchers to evaluate their theories and validate existing ones. 
Additionally, making research data available in combination with analysis code allows others to evaluate and replicate results reported in journal articles, improving the credibility of science.
In many circumstances, sharing research data bears a risk of disclosing sensitive attributes of the individuals that comprise the data.
In fact, privacy constraints have been named among the biggest hurdles in the advancement of computational social science [@lazer_css_2009], and among top reasons for companies to not share their data with researchers [@fpf_2017]. 
To overcome these obstacles, data collectors can employ a suite of different disclosure limitation techniques when sharing data, ranging to altering some values (i.e., through top-coding, record-swapping or adding noise) to creating entirely synthetic data [SEE DE WAAL FOR AN OVERVIEW; ALSO DRECHSLER BOOK]. 
In principle, all of these techniques alter the data, and limit its quality to some extent.
That is, all disclosure limitation techniques reduce the amount of information in the data to protect the privacy of the individuals in the data. 
The million-dollar question is how useful the altered data is relative to the original.

Privacy preservation

Synthetic data

Assessment of utility is of utmost importance

1. Utility measures can guide finetuning the synthesis models

2. Data users want to know what the synthetic data can and can't be used for

__Section 2: problem__

Measuring utility is hard.

Specific utility measures vs. general utility measures

1. Applying specific utility measures can be hard, as data disseminators generally do not know which analyses will be performed with the data

2. Specific utility measures tend to be too narrow: high utility on one reconstructed analysis may say close to nothing about another analysis

3. Global utility measures may be too broad: important deviations might be missed and a model that is reasonably good in general may be not too good for specific analyses. 

4. Global utility measures are hard to interpret, and say little about the regions in which the synthetic data do not resemble the true data accurately enough.

__Section 3: current state of the field__ (maybe switch 2\&3)

Current state of the literature

Ways to assess specific utility (confidence interval overlap; ellipsoidal (i.e., multivariate posterior distribution) overlap; univariate data visualizations)

Ways to assess general utility

1. $pMSE$ (explain, then problem: which model to use; results can vary depending on the model that is used to evaluate the synthetic data)

2. Kullback-Leibler divergence (only suitable for normally distributed data).

__Section 4: our contribution__

1. Introduce density ratio methods to compare densities of observed and synthetic sets.

- Maybe also incorporate something about density estimation methods (Wasserman; I have to read more about this first)

2. Bridge the gap between specific and general utility measures, by using dimension reduction techniques to visualize the discrepancies between the observed and synthetic data. 

3. Perform a simulation study to give indications about which methods to use (think about how to do this). 

4. Implement all this in an R-package

__Section 5: outline article__

# Methodology

__Section 1: density estimation__

Decide on whether density estimation will be included.

__Section 2: density ratio estimation__

Apparently this is easier than density estimation.

Check section 10.2 in density ratio estimation in machine learning. 
Two-sample test/homogeneity test (Kullback, 1959): test whether probability distributions be behind two sets of samples are equivalent.

"A standard approach to the two-sample test is to estimate a divergence between two probability distributions (Keziou & Leoni-Aubin, 2005; Kanamori et al., 2011a). A key observation is that a general class of divergences (Ali & Silvey, 1966; Csisz√°r, 1967), including the Kullback-Leibler divergene (Kullback & Leibler, 1951) and the Pearson divergence (Pearson 1900) can be approximated accurately via density-ratio estimation (Nguyen et al., 2010; Sugiyama et al., 2011c), resulting in better test accuracy than estimating the distributions separately."

__Section 3: theoretical comparison with conventional approaches for general utility assessment__

Shortly detail $pMSE$ and eventually Kullback-Leibler divergence. 

Explain in what sense the approaches we introduce are different.

__Section 4: Dimension reduction for visualizations__

Explain techniques in more detail (maybe start from PCA; go to UMAP/other advanced techniques).

How does this follow from density estimation?

# Simulations

1. Simple, multivariate normal simulation (e.g., two correlation structures, two sample sizes, so $2 \times 2$ full factorial design)

2. More advanced simulation (e.g., some non-linearities, different sample sizes)

Have to think about this in more detail still.

# Real data example

Clinical records heart-failure data?









Current ways to assess the utility?

- pMSE - logistic, regression, CART models (Snoke, Raab, Nowok, Dibben & Slavkovic, 2018; General and specific utility measures for synthetic data AND Woo, Reiter, Oganian & Karr, 2009; Global measures of data utlity for microdata masked for disclosure limitation)

- Kullback-Leiber divergence (Karr, Kohnen, Oganian, Reiter & Sanil, 2006; A framework for evaluating the utility of data altered to protect confidentiality). 

- According to multiple authors, both specific and general utility measures have important drawbacks (see Drechsler Utility PSD; cites others). Narrow measures potentially focus on analyses that are not relevant for the end user, and do not generalize to the analyses that are relevant. Global utility measures are generally too broad, and important deviations in the synthetic data might be missed. Moreover, the measures are typically hard to interpret. 

- See Drechsler for a paragraph on fit for purpose measures, that lie between general and specific utility measures (i.e., plausibility checks such as non-negativity; goodness of fit measures as $\chi^2$ for cross-tabulations; Kolmogorov-Smirnov).

- Drechsler also illustrates that the standardized $pMSE$ has substantial flaws, as the results are highly dependent on the model used to estimate the propensity scores, and unable to detect important differences in the utility for most of the model specifications. Hence, it is claimed that a thorough assessment of utility is required. 

# Methodology

TO DO

# Simulations

TO DO

# Real data example

TO DO

# Results

TO DO

# Discussion and conclusion

TO DO


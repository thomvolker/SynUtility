---
title: "Can the $pMSE$ detect missing relationships between variables?"
format: html
date: last-modified
author: Thom Benjamin Volker
---

The answer is: Yes, but only if the $pMSE$ model is well-specified, which increases the burden on behalf of the user. 

# Properties $pMSE$ for CART, logistic regression and lasso regression

```{r}
#| message: false
#| warning: false
library(synthpop)
library(patchwork)
library(purrr)
library(dplyr)
library(glmnet)
library(ggplot2)
library(lfda)
```


## Generate data

### Three predictor variables

```{r}
set.seed(123)

N <- 500
r <- 0.5

P3 <- 3
M3 <- rep(0, P3)

SR3 <- matrix(r, P3, P3)
diag(SR3) <- 1

SS3 <- diag(P3)

XR3 <- rnorm(N*P3) |> matrix(N, P3) %*% chol(SR3)
XS3 <- rnorm(N*P3) |> matrix(N, P3) %*% chol(SS3)
```

### Ten predictor variables

```{r}
P10 <- 10
M10 <- rep(0, P10)

SR10 <- matrix(r, P10, P10)
diag(SR10) <- 1

SS10 <- diag(P10)

XR10 <- rnorm(N*P10) |> matrix(N, P10) %*% chol(SR10)
XS10 <- rnorm(N*P10) |> matrix(N, P10) %*% chol(SS10)
```

### Twenty predictor variables

```{r}
P20 <- 20
M20 <- rep(0, P20)

SR20 <- matrix(r, P20, P20)
diag(SR20) <- 1

SS20 <- diag(P20)

XR20 <- rnorm(N*P20) |> matrix(N, P20) %*% chol(SR20)
XS20 <- rnorm(N*P20) |> matrix(N, P20) %*% chol(SS20)
```

### Combine data sets

```{r}
XR <- list(XR3, XR10, XR20)
XS <- list(XS3, XS10, XS20)
```


## Compare real and synthetic data

```{r}
#| cache: true
#| message: false
CART <- map2_dbl(XR, XS,
                 ~synthpop::utility.gen(data.frame(.x),
                                        data.frame(.y),
                                        method = "cart",
                                        print.flag=F)$S_pMSE)


logistic <- map2_dbl(XR, XS,
                     ~synthpop::utility.gen(data.frame(.x),
                                            data.frame(.y),
                                            method = "logit",
                                            print.flag=F,
                                            maxorder = 1)$S_pMSE)

lasso <- map2_dbl(XR, XS,
                  function(real, synthetic) {
                    form <- paste0("Synthetic ~ .^3 + ", 
                                   paste0("I(X", 1:ncol(real), "^2)", collapse = " + "),
                                   " + ",
                                   paste0("I(X", 1:ncol(real), "^3)", collapse = " + "))
                    dat <- bind_rows(Real = data.frame(real),
                                     Synthetic = data.frame(synthetic),
                                    .id = "Synthetic")
                    X   <- model.matrix(as.formula(form), dat)[,-1]
                    Y   <- factor(dat$Synthetic)
                    mod <- cv.glmnet(x = X, y = Y, alpha = 1, family = "binomial")
                    p   <- predict(mod, newx = X, s = mod$lambda.min, type = "response")
                    mean((p-0.5)^2) / ((ncol(X))*0.5^3/nrow(X))
                  })

data.frame(CART = CART,
           Logistic = logistic,
           Lasso = lasso) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

So, we can conclude that the $pMSE$ is not very good at detecting missed relationships between variables, and the problem gets worse when the dimensionality of the data increases. Note that the estimates of the lasso are conservative, as the number of variables in the predictor matrix is chosen to be the number of parameters in the model (and thus shrinkage is not accounted for). Using a (probably) overly optimistic estimate (only the non-zero coefficients), the standardized $pMSE$ would be around $7$.

```{r}
map2(XR, XS, ~utility.tables(data.frame(.x), data.frame(.y)))
```

According to the figures, the same findings hold for two-way assessments of utility (i.e., for pairs of variables).

# Distributional similarity after dimension reduction

## Principal component analysis

### Kolmogorov-Smirnov test after PCA

Using the Kolmogorov-Smirnov test, we assess the distributional similarity of the real and synthetic data on the first principal component. 

```{r}
pca <- function(real, synthetic) {
  X   <- rbind(real, synthetic) |> scale()
  USV <- svd(X)
  ID  <- rep(c("Real", "Synthetic"), times = c(nrow(real), nrow(synthetic)))
  data.frame(X %*% USV$v, ID) |>
    setNames(c(paste0("PC", 1:ncol(USV$v)), "Synthetic"))
}

map2(XR, XS, function(real, synthetic) {
       pca(real, synthetic) |>
         ggplot(aes(PC1, PC2, col = Synthetic)) +
         geom_point() +
         theme_minimal()
     }) |>
  patchwork::wrap_plots(ncol = 3, guides = "collect") &
  theme(legend.position = "bottom")

map2(XR, XS, function(real, synthetic) {
  D <- pca(real, synthetic)
  ks.test(D |> filter(Synthetic == "Real") |> pull(PC1),
          D |> filter(Synthetic == "Synthetic") |> pull(PC1))
})
```


### $pMSE$ after PCA

A similar result can be obtained by calculating the $pMSE$ using the first-order interactions (including squared terms) for the first two principal components. A similar result here implies a $pMSE$ that also indicates model misfit.


```{r}
map2(XR, XS, function(real, synthetic) {
  PC <- pca(real, synthetic)
  R <- PC |> filter(Synthetic == "Real")
  S <- PC |> filter(Synthetic == "Synthetic")
  
  Synthpop_S_pMSE <- utility.gen(R |> select(PC1, PC2),
                                 S |> select(PC1, PC2),
                                 method = "logit",
                                 maxorder = 1)$S_pMSE
  
  S_prop <- nrow(S) / nrow(PC)
  n_pMSE <- 5 * (1 - S_prop)^2 * S_prop / nrow(PC)
  Manual_S_pMSE <- PC |>
    select(PC1, PC2, Synthetic) |>
    glm(formula = factor(Synthetic) ~ PC1 + PC2 + PC1:PC2 + I(PC1^2) + I(PC2^2),
        family = binomial) |>
    predict(type = "response") |>
    {\(x) mean((x-S_prop)^2) / n_pMSE}()
  
  c(Synthpop = Synthpop_S_pMSE,
    Manual = Manual_S_pMSE)
})

```

We see a clear discrepancy between the real and synthetic data in the figure, that is also reflected in the discrepancy score of the Kolmogorov-Smirnov test, but not in the logistic regression model (admittedly with only first order interaction terms and without squared terms, because that is not allowed by the software). If we include these terms, we find a very large standardized $pMSE$ value.

Of course, this finding makes sense, as we see that values that score high on the first principal component (in an absolute sense, so the values in the tails of the distribution) are more likely to be "real" values, whereas values that score high on the second principal component (in an absolute sense, again the values in the tails of the distribution) are much more likely to be synthetic records. However, this will only show up when considering the squared (or absolute) terms, because the centers of the distribution are at the same location for the real and synthetic data.


## Local Fisher discriminant analysis

```{r}
map2(XR, XS, function(real, synthetic) {
  X <- rbind(real, synthetic)
  Y <- rep(c("Real", "Synthetic"), times = c(nrow(real), nrow(synthetic)))
  Z <- lfda(X, Y, r = ncol(X), metric = "orthonormalized")$Z |>
    data.frame() |>
    setNames(paste0("D", 1:ncol(real)))
  
  Z |>
    ggplot(aes(x = D1, y = D2, col = Y)) +
    geom_point(alpha = 0.5) +
    theme_minimal()
}) |>
  patchwork::wrap_plots(ncol = 3, guides = 'collect') &
  theme(legend.position = "bottom")
```

First, note that in this situation LFDA gives a result that is highly similar to the result obtained with PCA. That is, the first two components (or dimensions) separate the observed and synthetic samples quite clearly.

### Kolmogorov-Smirnov test after LFDA

```{r}
out_lfda <- map2(XR, XS, function(real, synthetic) {
  X <- rbind(real, synthetic)
  Y <- rep(c("Real", "Synthetic"), times = c(nrow(real), nrow(synthetic)))
  lfda(X, Y, r = ncol(X), metric = "orthonormalized")$Z |>
    data.frame() |>
    setNames(paste0("D", 1:ncol(X))) |>
    mutate(Synthetic = factor(Y))
})

map(out_lfda, function(x) {
  R <- x |> filter(Synthetic == "Real") |> pull(D1)
  S <- x |> filter(Synthetic == "Synthetic") |> pull(D1)
  ks.test(R, S)
})
```

The result is very similar to the result of the K-S test after PCA, that is, there is quite a mismatch between the real and synthetic data. 

### $pMSE$ after LFDA

```{r}
map(out_lfda, function(x) {
  R <- x |> filter(Synthetic == "Real") |> select(D1, D2)
  S <- x |> filter(Synthetic == "Synthetic") |> select(D1, D2)
  
  Synthpop_S_pMSE <- utility.gen(R, S,
                                 method = "logit",
                                 maxorder = 1)$S_pMSE
  
  S_prop <- nrow(S) / (nrow(S) + nrow(R))
  n_pMSE <- 5 * (1 - S_prop)^2 * S_prop / (nrow(S) + nrow(R))
  Manual_S_pMSE <- x |>
    select(D1, D2, Synthetic) |>
    glm(formula = factor(Synthetic) ~ D1 + D2 + D1:D2 + I(D1^2) + I(D2^2),
        family = binomial) |>
    predict(type = "response") |>
    {\(x) mean((x-S_prop)^2) / n_pMSE}()
  
  c(Synthpop = Synthpop_S_pMSE,
    Manual = Manual_S_pMSE)
})
``` 

A similar result holds after calculating the $pMSE$ with all first-order interactions (including squared terms), there is a mismatch between the real and synthetic data. However, this mismatch shows up only after specifying the $pMSE$ model, which is sensitive to its specification. 

# Conclusion 

The $pMSE$ model works for missed relationships between variables, but only after projecting the original data on a lower-dimensional space, making use of these relationships. Using the entire data set, the $pMSE$ gives no satisfactory results. Also, after using dimension reduction techniques, the $pMSE$ model must be correctly specified.
If this is the case (i.e., when the model is specified such that the discrepancies between the real and synthetic data can be modeled as conditional mean differences), and when the dimensionality of the data is reduced, the $pMSE$ works adequately. However, it can be hard to specify this model, and it is certainly an ad hoc measure. Moreover, a non-parametric `CART` model is also not very good at picking up such discrepancies, leaving it a labor-intensive task to come up with a good evaluation model. 

In these instances, the Kolmogorov-Smirnov test provided an easy-to-use alternative that requires little specification, but depends on correctly reducing the dimensionality of the problem in such a way that the two data sets are maximally different on one component. Preferably, this single component is the first component, to only having to test the first component, rather than all $p$ components. 
